{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# KUC, PRG210\n",
    "#P9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>publish_date</th>\n",
       "      <th>headline_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20030219</td>\n",
       "      <td>aba decides against community broadcasting lic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20030219</td>\n",
       "      <td>act fire witnesses must be aware of defamation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20030219</td>\n",
       "      <td>a g calls for infrastructure protection summit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz staff in aust strike for pay rise</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20030219</td>\n",
       "      <td>air nz strike to affect australian travellers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   publish_date                                      headline_text\n",
       "0      20030219  aba decides against community broadcasting lic...\n",
       "1      20030219     act fire witnesses must be aware of defamation\n",
       "2      20030219     a g calls for infrastructure protection summit\n",
       "3      20030219           air nz staff in aust strike for pay rise\n",
       "4      20030219      air nz strike to affect australian travellers"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "path='abcnews-date-text.csv'\n",
    "pd.read_csv(path).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af3caca7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1226258 entries, 0 to 1226257\n",
      "Data columns (total 2 columns):\n",
      " #   Column         Non-Null Count    Dtype \n",
      "---  ------         --------------    ----- \n",
      " 0   publish_date   1226258 non-null  int64 \n",
      " 1   headline_text  1226258 non-null  object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 18.7+ MB\n"
     ]
    }
   ],
   "source": [
    "newsData = pd.read_csv(path, encoding='ascii')\n",
    "newsData.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa13533f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## First, lets filter our dataset and make a reduced version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf9f8a75",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      aba decides against community broadcasting lic...\n",
       "1         act fire witnesses must be aware of defamation\n",
       "2         a g calls for infrastructure protection summit\n",
       "3               air nz staff in aust strike for pay rise\n",
       "4          air nz strike to affect australian travellers\n",
       "                             ...                        \n",
       "995                  conference to focus on tuna fishery\n",
       "996                        council hosts farewell for mp\n",
       "997                  council resists eba roster pressure\n",
       "998                     customs house restoration opened\n",
       "999                dam water levels still critically low\n",
       "Name: headline_text, Length: 1000, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsData.headline_text[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5eb2411",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#a mask filters data by defining boolean parameters\n",
    "mask=np.array([bool(round(item*0.5010))for item in np.random.rand(newsData.shape[0],).tolist()])\n",
    "\n",
    "newsData_reduced= newsData[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac4e63d8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2433\n"
     ]
    }
   ],
   "source": [
    "print(len(newsData_reduced))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30f8e0a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Useful libraries for tracking processing progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "def2d42f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from tqdm.notebook import trange, tqdm\n",
    "\n",
    "tqdm.pandas(desc='Preprocessing text')\n",
    "\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stops=stopwords.words('english')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3eed79a8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessing(corpus):\n",
    "    out=[word.strip().lower() for word in word_tokenize(corpus) if word not in stops and word.isalnum() ]\n",
    "    out=' '.join(out)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8d6c09",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### .progress_apply from tqdm same as .apply but with progress bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9447272f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b98706651604d7f81e46544bb451d73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing text:   0%|          | 0/2433 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jw/pk9yd5v978d_6j_wky4ptw480000gn/T/ipykernel_53650/1172714680.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  newsData_reduced['headline_text'] = newsData_reduced.headline_text.progress_apply(lambda x: preprocessing(x))\n"
     ]
    }
   ],
   "source": [
    "newsData_reduced['headline_text'] = newsData_reduced.headline_text.progress_apply(lambda x: preprocessing(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "05516823",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dfdb567c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def vectorGenerator(documents_preprocessed):\n",
    "    from datetime import datetime\n",
    "    beg = datetime.now()\n",
    "    \n",
    "    vectorizer = CountVectorizer() \n",
    "    \n",
    "    bow = vectorizer.fit_transform(documents_preprocessed)  #note that you must use the preprocessed documents in string \n",
    "                                                                #form for this function/method\n",
    "    end = datetime.now()\n",
    "    \n",
    "    print('documents processed:',bow.shape[0])\n",
    "    print('time spent:',end-beg)\n",
    "    return bow, vectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "005535bb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents processed: 2433\n",
      "time spent: 0:00:00.091134\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<2433x5555 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 13051 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_reduced, bow_vectorizer = vectorGenerator(newsData_reduced['headline_text'])\n",
    "bow_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db7e3c5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Cosine similarity on reduced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "747c1e38",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample headline:\n",
      "torres strait islanders battle league cup\n",
      "\n",
      "---------------------------------------\n",
      "\n",
      "The most similar to sample headline:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'land battle cape barren islanders'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_sim =cosine_similarity(bow_reduced)\n",
    "\n",
    "#setting all the \"self to self comparisons\" to zero so they dont show up as the best match\n",
    "\n",
    "for i in range(cosine_sim.shape[0]):\n",
    "    cosine_sim[i,i] = 0\n",
    "\n",
    "# Get the most similar item for random sampled row \n",
    "\n",
    "row_idx = np.random.randint(cosine_sim.shape[0])\n",
    "most_similar = np.argmax(cosine_sim[row_idx])\n",
    "\n",
    "    \n",
    "print('Sample headline:')\n",
    "\n",
    "print(newsData_reduced['headline_text'].iloc[row_idx])\n",
    "print('\\n---------------------------------------\\n')\n",
    "print('The most similar to sample headline:')\n",
    "\n",
    "newsData_reduced.headline_text.iloc[(most_similar)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adaa9d4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Now, lets work with the complete dataframe of 1226258 headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "233eb0f2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def preProcessingParallel(document):\n",
    "    \"\"\"A function performing all pre-processing steps\n",
    "    on ONE document\"\"\"\n",
    "    \n",
    "    \n",
    "    #removing stopwords and whitespace and punctuation/\n",
    "    #special characters\n",
    "    documentNonstop = (' ').join([\n",
    "            re.sub('[^A-Za-z0-9æøå]','',w.strip()) \n",
    "            for w in word_tokenize(document) \n",
    "            if not w.strip() in stops and w.isalnum()\n",
    "                        ])\n",
    "    \n",
    "    return documentNonstop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c073643c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8a746dfee54f11bbd7c8368323aafe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1226258 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    2.2s\n",
      "[Parallel(n_jobs=-1)]: Done 188 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=-1)]: Done 18444 tasks      | elapsed:    2.7s\n",
      "[Parallel(n_jobs=-1)]: Done 434188 tasks      | elapsed:    8.2s\n",
      "[Parallel(n_jobs=-1)]: Done 1024012 tasks      | elapsed:   16.1s\n",
      "[Parallel(n_jobs=-1)]: Done 1226258 out of 1226258 | elapsed:   19.0s finished\n"
     ]
    }
   ],
   "source": [
    "newsData_preprocessed_full = Parallel(n_jobs=-1,verbose=3)(\n",
    "    delayed(preProcessingParallel)(document) for document in tqdm(newsData.headline_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b7273151",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def TfIdfvectorGenerator(documents_preprocessed):\n",
    "    \"\"\" Taking a corpus of documents which have been preprocessed and exist in string shape, \n",
    "    creates TFIDF representations and returns the representations, and the TFIDF vectorizer\"\"\"\n",
    "    from datetime import datetime\n",
    "    beg = datetime.now()\n",
    "    vectorizer = TfidfVectorizer(min_df=1,\n",
    "                                 max_df=0.75) \n",
    "    \n",
    "    tfidf = vectorizer.fit_transform(documents_preprocessed)  #note that you must use the preprocessed documents in string \n",
    "                                                                #form for this function/method\n",
    "    end = datetime.now()\n",
    "    \n",
    "    print('documents processed:',tfidf.shape[0])\n",
    "    print('time spent:',end-beg)\n",
    "    return tfidf, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7be161ec",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents processed: 1226258\n",
      "time spent: 0:00:06.104343\n"
     ]
    }
   ],
   "source": [
    "tfidf, vectorizer = TfIdfvectorGenerator(newsData_preprocessed_full) #The vectorizer from sklearn is already fast (no need for parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a25d6af8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b7c8889e3994214b0e907ee7b68cb60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   8 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=-1)]: Done 488 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=-1)]: Done 9192 tasks      | elapsed:    3.1s\n",
      "[Parallel(n_jobs=-1)]: Done 23528 tasks      | elapsed:    7.1s\n",
      "[Parallel(n_jobs=-1)]: Done 41960 tasks      | elapsed:   12.3s\n",
      "[Parallel(n_jobs=-1)]: Done 64488 tasks      | elapsed:   18.6s\n",
      "[Parallel(n_jobs=-1)]: Done 91112 tasks      | elapsed:   26.0s\n",
      "[Parallel(n_jobs=-1)]: Done 121832 tasks      | elapsed:   34.7s\n",
      "[Parallel(n_jobs=-1)]: Done 156648 tasks      | elapsed:   44.5s\n",
      "[Parallel(n_jobs=-1)]: Done 195560 tasks      | elapsed:   55.3s\n",
      "[Parallel(n_jobs=-1)]: Done 238568 tasks      | elapsed:  1.1min\n",
      "[Parallel(n_jobs=-1)]: Done 285672 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=-1)]: Done 336872 tasks      | elapsed:  1.6min\n",
      "[Parallel(n_jobs=-1)]: Done 392168 tasks      | elapsed:  1.8min\n",
      "[Parallel(n_jobs=-1)]: Done 451560 tasks      | elapsed:  2.1min\n",
      "[Parallel(n_jobs=-1)]: Done 515048 tasks      | elapsed:  2.4min\n",
      "[Parallel(n_jobs=-1)]: Done 582632 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 654312 tasks      | elapsed:  3.1min\n",
      "[Parallel(n_jobs=-1)]: Done 730088 tasks      | elapsed:  3.4min\n",
      "[Parallel(n_jobs=-1)]: Done 809960 tasks      | elapsed:  3.8min\n",
      "[Parallel(n_jobs=-1)]: Done 893928 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 981992 tasks      | elapsed:  4.6min\n",
      "[Parallel(n_jobs=-1)]: Done 1074152 tasks      | elapsed:  5.0min\n",
      "[Parallel(n_jobs=-1)]: Done 1170408 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1226258 out of 1226258 | elapsed:  5.7min finished\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcd56fd4f8264fb1a64979ab5c4ce727",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1226258 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "row_idx = np.random.randint(newsData.shape[0])\n",
    "\n",
    "cosineDistances = Parallel(n_jobs=-1,verbose=3)(\n",
    "    delayed(cosine_similarity)(tfidf[row_idx]*tfidf.shape[0],item) for item in tqdm(tfidf))\n",
    "cosineDistances = np.array([item[0][0] for item in tqdm(cosineDistances)]) # Expect cosine_similarity to take a while"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2eb01a55",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 732599, 1219540,  983766,  915378, 1214580,  366878,  645141,\n",
       "        902226,  902506,  969005])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_similar_indices = cosineDistances.argsort()[-11:][::-1][1:] #argsort is like argmax, but retr\n",
    "most_similar_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9284be61",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+==============\t+==============\t+============================================+\n",
      "| Sample: \t| 351474 \t| alcohol bans proposed for east arnhem land\n",
      "+==============\t+==============\t+============================================+\n",
      "| Match 1 \t| 732599 \t| mixed messages on alcohol bans gooda arnhem land\n",
      "+--------------\t+--------------\t+--------------------------------------------+\n",
      "| Match 2 \t| 1219540 \t| on tour in arnhem land\n",
      "+--------------\t+--------------\t+--------------------------------------------+\n",
      "| Match 3 \t| 983766 \t| east arnhem land development nhulunbuy\n",
      "+--------------\t+--------------\t+--------------------------------------------+\n",
      "| Match 4 \t| 915378 \t| creating farms in east arnhem land\n",
      "+--------------\t+--------------\t+--------------------------------------------+\n",
      "| Match 5 \t| 1214580 \t| new school proposed for garma site east arnhem\n",
      "+--------------\t+--------------\t+--------------------------------------------+\n",
      "| Match 6 \t| 366878 \t| east arnhem alcohol system hits snags\n",
      "+--------------\t+--------------\t+--------------------------------------------+\n",
      "| Match 7 \t| 645141 \t| arnhem land leaders meet\n",
      "+--------------\t+--------------\t+--------------------------------------------+\n",
      "| Match 8 \t| 902226 \t| abbott to tour remote north east region of arnhem land\n",
      "+--------------\t+--------------\t+--------------------------------------------+\n",
      "| Match 9 \t| 902506 \t| abbott to camp in arnhem land\n",
      "+--------------\t+--------------\t+--------------------------------------------+\n",
      "| Match 10 \t| 969005 \t| 2015 garma festival in north east arnhem land\n",
      "+--------------\t+--------------\t+--------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "print('+==============\\t+==============\\t+============================================+')\n",
    "print('| Sample: \\t|',row_idx,'\\t|',newsData.loc[row_idx,'headline_text'])\n",
    "print('+==============\\t+==============\\t+============================================+')\n",
    "i=0\n",
    "for item in most_similar_indices:\n",
    "    i+=1\n",
    "    print('| Match',i,'\\t|',item,'\\t|',newsData.loc[item,'headline_text'])\n",
    "    print('+--------------\\t+--------------\\t+--------------------------------------------+')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2118adee",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Let the LDiA try to group into topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "422e0b8d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "LDiA = LatentDirichletAllocation(n_components=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381e04b6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Note that this will take A WHILE with all the 1.25 million documents. Use the reduced vectors instead for testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2722dfb5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lda_matrix = LDiA.fit_transform(bow_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c2839557",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lda_components = LDiA.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c6eb6b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Examining the topic scores for document 0 in our reduced sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1cddcc66",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03400931, 0.2841111 , 0.03360859, 0.61425408, 0.03401693])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0a8c862e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5555)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_components.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e972ccbc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.19996296, 1.19778701, 2.20288595, ..., 0.20001655, 1.19997507,\n",
       "       1.20070848])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_components[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "31c0f347",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['08', '10', '100', ..., 'zone', 'zoning', 'zoo'], dtype=object)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary = bow_vectorizer.get_feature_names_out() #getting vocabulary out of TfidfVectorizer\n",
    "vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e07229",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Creating lists of words associated with each topic so we can examine the topics and figure out if they make sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "07075450",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:  ['police', 'us', 'says', 'man', 'get', 'australia', 'face', 'record', 'rise', 'hunt', 'north', 'nsw', 'two']\n",
      "Topic 1:  ['council', 'call', 'govt', 'vic', 'says', 'fire', 'rise', 'change', 'report', 'election', 'detention', 'abuse', 'sydney']\n",
      "Topic 2:  ['police', 'man', 'victoria', 'group', 'queensland', 'council', 'hit', 'australia', 'crash', 'calls', 'flood', 'fire', 'south']\n",
      "Topic 3:  ['new', 'police', 'man', 'court', 'interview', 'farmers', 'plan', 'charged', 'fire', 'case', 'murder', 'claims', 'abc']\n",
      "Topic 4:  ['day', 'government', 'crash', 'man', 'canberra', 'drought', 'water', 'new', 'council', 'us', 'iraq', 'melbourne', 'sydney']\n"
     ]
    }
   ],
   "source": [
    "topics = []\n",
    "\n",
    "for index, component in enumerate(lda_components):\n",
    "    zipped = zip(vocabulary, component)\n",
    "    top_terms_key = sorted(zipped, key = lambda t: t[1], reverse=True)[:13]\n",
    "    top_terms_list = list(dict(top_terms_key).keys())\n",
    "    topics.append(top_terms_list)\n",
    "    print(\"Topic \"+str(index)+\": \",top_terms_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4745e0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "doc=0\n",
    "topics[np.argmax(lda_matrix[doc])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b97f8cd",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tfidf_reduced, tfidf_vectorizer = TfIdfvectorGenerator(newsData_reduced['headline_text'])\n",
    "tfidf_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce70661",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lda_tfidf_matrix= LDiA.fit_transform(tfidf_reduced)\n",
    "lda_tfidf_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8fc86c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lda_components = LDiA.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bb03a6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "topics = []\n",
    "\n",
    "for index, component in enumerate(lda_components):\n",
    "    zipped = zip(vocabulary, component)\n",
    "    top_terms_key = sorted(zipped, key = lambda t: t[1], reverse=True)[:13]\n",
    "    top_terms_list = list(dict(top_terms_key).keys())\n",
    "    topics.append(top_terms_list)\n",
    "    print(\"Topic \"+str(index)+\": \",top_terms_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b878bfbb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_top_words(model, feature_names, n_top_words, title):\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(30, 15), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        top_features_ind = topic.argsort()[: -n_top_words - 1 : -1]\n",
    "        top_features = [feature_names[i] for i in top_features_ind]\n",
    "        weights = topic[top_features_ind]\n",
    "\n",
    "        ax = axes[topic_idx]\n",
    "        ax.barh(top_features, weights, height=0.7)\n",
    "        ax.set_title(f\"Topic {topic_idx +1}\", fontdict={\"fontsize\": 30})\n",
    "        ax.invert_yaxis()\n",
    "        ax.tick_params(axis=\"both\", which=\"major\", labelsize=20)\n",
    "        for i in \"top right left\".split():\n",
    "            ax.spines[i].set_visible(False)\n",
    "        fig.suptitle(title, fontsize=40)\n",
    "\n",
    "    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d27f114",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(\n",
    "    n_components=10,\n",
    "    max_iter=5,\n",
    "    learning_method=\"online\",\n",
    "    learning_offset=50.0,\n",
    "    random_state=0,\n",
    ")\n",
    "t0 = time()\n",
    "lda.fit(bow_reduced)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "bow_feature_names = bow_vectorizer.get_feature_names_out()\n",
    "plot_top_words(lda, bow_feature_names, 10 , \"Topics in LDA model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22113c5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lda = LatentDirichletAllocation(\n",
    "    n_components=10,\n",
    "    max_iter=5,\n",
    "    learning_method=\"online\",\n",
    "    learning_offset=50.0,\n",
    "    random_state=0,\n",
    ")\n",
    "t0 = time()\n",
    "lda.fit(tfidf_reduced)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "plot_top_words(lda, tfidf_feature_names, 10 , \"Topics in LDA model using tf-idf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2804058c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
